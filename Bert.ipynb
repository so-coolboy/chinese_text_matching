{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wMiqQ1t0O2t",
        "outputId": "3bd3504c-d580-451e-9f6a-e477fc657715"
      },
      "source": [
        "%tensorflow_version 1.x\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hL3Yo1hFkcPO",
        "outputId": "bcbcaea4-6f08-40ed-ca21-fa822dd67a2a"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyjj9vtJarRv",
        "outputId": "946b4a9a-e610-44b1-d3f1-1eacd3ac0ab4"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-08 13:26:43--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.7.144, 172.217.7.208, 172.217.13.240, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.7.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M  86.2MB/s    in 8.3s    \n",
            "\n",
            "2021-02-08 13:26:51 (76.1 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb4yDOIGcs1l",
        "outputId": "d0c6cd13-8b7b-4a3c-eef3-81719e1e54dc"
      },
      "source": [
        "!unzip /content/multi_cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_EgeNTrgQsJ",
        "outputId": "10e83f07-a9f2-48c8-ac95-e52b2509aff2"
      },
      "source": [
        "!git clone https://github.com/terrifyzhao/text_matching.git\r\n",
        "!cp -r /content/text_matching/input ./"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'text_matching'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Total 334 (delta 0), reused 0 (delta 0), pack-reused 334\u001b[K\n",
            "Receiving objects: 100% (334/334), 24.77 MiB | 30.23 MiB/s, done.\n",
            "Resolving deltas: 100% (212/212), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-yFyqnTgqKH",
        "outputId": "88e80db4-8fc4-414b-e85b-51891ae4246e"
      },
      "source": [
        "!pip install bert4keras"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert4keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/1b/5862d5b29a6b0aa2684299ec9af31616fcec52608573fc9399aec365161d/bert4keras-0.10.0.tar.gz (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30kB 24.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 40kB 22.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras<=2.3.1 in /tensorflow-1.15.2/python3.6 (from bert4keras) (2.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras<=2.3.1->bert4keras) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.1.2)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.10.0-cp36-none-any.whl size=44153 sha256=595e204507a0a515623431230970508ab36c60577dbfa26af045dfc397c6d102\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ca/95/591f4a3c5759f4e90ba75511ff6cd27a41adb16af57e63c822\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: bert4keras\n",
            "Successfully installed bert4keras-0.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aJS_X3zgmKx",
        "outputId": "1d3caee9-061c-41e2-fe21-4962e6ededfe"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from bert4keras.backend import keras, set_gelu, K\r\n",
        "from bert4keras.tokenizers import Tokenizer\r\n",
        "from bert4keras.models import build_transformer_model\r\n",
        "from bert4keras.optimizers import Adam\r\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\r\n",
        "from bert4keras.snippets import open\r\n",
        "from keras.layers import Dropout, Dense"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA01G2GSgxcQ"
      },
      "source": [
        "set_gelu('tanh')  # 切换gelu版本\r\n",
        "\r\n",
        "maxlen = 64\r\n",
        "batch_size = 8\r\n",
        "config_path = '/content/multi_cased_L-12_H-768_A-12/bert_config.json'\r\n",
        "checkpoint_path = '/content/multi_cased_L-12_H-768_A-12/bert_model.ckpt'\r\n",
        "dict_path = '/content/multi_cased_L-12_H-768_A-12/vocab.txt'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqHWpetwhEGZ"
      },
      "source": [
        "def load_data(filename):\r\n",
        "    df = pd.read_csv(filename,header=0,encoding='utf8')\r\n",
        "    f = df.values\r\n",
        "    D = []\r\n",
        "    # with open(filename, encoding='utf-8') as f:\r\n",
        "    for i,l in enumerate(f):\r\n",
        "        # print(l)\r\n",
        "        text1, text2, label = l#.strip().split(',')\r\n",
        "        D.append((text1, text2, int(label)))\r\n",
        "    return D\r\n",
        "\r\n",
        "\r\n",
        "# 加载数据集\r\n",
        "train_data = load_data('/content/input/train.csv')\r\n",
        "valid_data = load_data('/content/input/dev.csv')\r\n",
        "test_data = load_data('/content/input/test.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmYpqWmGhOha",
        "outputId": "6a728062-b60c-4bfe-c8d9-54e4208d2efa"
      },
      "source": [
        "# 建立分词器\r\n",
        "tokenizer = Tokenizer(dict_path, do_lower_case=True)\r\n",
        "\r\n",
        "\r\n",
        "class data_generator(DataGenerator):\r\n",
        "    \"\"\"数据生成器\r\n",
        "    \"\"\"\r\n",
        "    def __iter__(self, random=False):\r\n",
        "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\r\n",
        "        for is_end, (text1, text2, label) in self.sample(random):\r\n",
        "            token_ids, segment_ids = tokenizer.encode(\r\n",
        "                text1, text2, maxlen=maxlen\r\n",
        "            )\r\n",
        "            batch_token_ids.append(token_ids)\r\n",
        "            batch_segment_ids.append(segment_ids)\r\n",
        "            batch_labels.append([label])\r\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\r\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\r\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\r\n",
        "                batch_labels = sequence_padding(batch_labels)\r\n",
        "                yield [batch_token_ids, batch_segment_ids], batch_labels\r\n",
        "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\r\n",
        "\r\n",
        "\r\n",
        "# 加载预训练模型\r\n",
        "bert = build_transformer_model(\r\n",
        "    model='bert',\r\n",
        "    config_path=config_path,\r\n",
        "    checkpoint_path=checkpoint_path,\r\n",
        "    with_pool=True,\r\n",
        "    return_keras_model=False,\r\n",
        ")\r\n",
        "\r\n",
        "output = Dropout(rate=0.1)(bert.model.output)\r\n",
        "output = Dense(\r\n",
        "    units=2, activation='softmax', kernel_initializer=bert.initializer\r\n",
        ")(output)\r\n",
        "\r\n",
        "model = keras.models.Model(bert.model.input, output)\r\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     (None, None, 768)    91812096    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "                                                                 Transformer-4-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "                                                                 Transformer-5-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "                                                                 Transformer-6-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "                                                                 Transformer-7-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "                                                                 Transformer-8-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "                                                                 Transformer-9-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "                                                                 Transformer-10-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "                                                                 Transformer-11-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Pooler (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "Pooler-Dense (Dense)            (None, 768)          590592      Pooler[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           Pooler-Dense[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 2)            1538        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 177,854,978\n",
            "Trainable params: 177,854,978\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4WO4RUhjpuY",
        "outputId": "d7f86a35-964c-4ba0-fe18-b0f92092305e"
      },
      "source": [
        "model.compile(\r\n",
        "    loss='sparse_categorical_crossentropy',\r\n",
        "    optimizer=Adam(2e-5),  # 用足够小的学习率\r\n",
        "    # optimizer=PiecewiseLinearLearningRate(Adam(5e-5), {10000: 1, 30000: 0.1}),\r\n",
        "    metrics=['accuracy'],\r\n",
        ")\r\n",
        "\r\n",
        "# 转换数据集\r\n",
        "train_generator = data_generator(train_data, batch_size)\r\n",
        "valid_generator = data_generator(valid_data, batch_size)\r\n",
        "test_generator = data_generator(test_data, batch_size)\r\n",
        "\r\n",
        "\r\n",
        "def evaluate(data):\r\n",
        "    total, right = 0., 0.\r\n",
        "    for x_true, y_true in data:\r\n",
        "        y_pred = model.predict(x_true).argmax(axis=1)\r\n",
        "        y_true = y_true[:, 0]\r\n",
        "        total += len(y_true)\r\n",
        "        right += (y_true == y_pred).sum()\r\n",
        "    return right / total\r\n",
        "\r\n",
        "\r\n",
        "class Evaluator(keras.callbacks.Callback):\r\n",
        "    def __init__(self):\r\n",
        "        self.best_val_acc = 0.\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        val_acc = evaluate(valid_generator)\r\n",
        "        if val_acc > self.best_val_acc:\r\n",
        "            self.best_val_acc = val_acc\r\n",
        "            model.save_weights('best_model.weights')\r\n",
        "        test_acc = evaluate(test_generator)\r\n",
        "        print(\r\n",
        "            u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\\n' %\r\n",
        "            (val_acc, self.best_val_acc, test_acc)\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "evaluator = Evaluator()\r\n",
        "model.fit_generator(\r\n",
        "    train_generator.forfit(),\r\n",
        "    steps_per_epoch=len(train_generator),\r\n",
        "    epochs=6,\r\n",
        "    callbacks=[evaluator]\r\n",
        ")\r\n",
        "\r\n",
        "model.load_weights('best_model.weights')\r\n",
        "print(u'final test acc: %05f\\n' % (evaluate(test_generator)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/6\n",
            "12500/12500 [==============================] - 2326s 186ms/step - loss: 0.3917 - accuracy: 0.8252\n",
            "val_acc: 0.81760, best_val_acc: 0.81760, test_acc: 0.80990\n",
            "\n",
            "Epoch 2/6\n",
            "  829/12500 [>.............................] - ETA: 35:52 - loss: 0.2667 - accuracy: 0.8910"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}