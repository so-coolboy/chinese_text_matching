{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiMPM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kHP4Vmy7YMb"
      },
      "source": [
        "# BiMPM算法实战"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3stRbT0S8fk5"
      },
      "source": [
        "## 数据准备\r\n",
        "数据来源于https://github.com/terrifyzhao/text_matching， 里面是中文的匹配数据，有sentence1，sentence2和对应的label，0代表不匹配，1代表匹配。可以直接在GitHub中引用出来"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1AsR2BFQ_9m",
        "outputId": "225dfe9d-0495-4772-9c04-2270f4bdf8d2"
      },
      "source": [
        "!git clone https://github.com/terrifyzhao/text_matching.git\r\n",
        "!cp -r /content/text_matching/input ./"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'text_matching'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Total 334 (delta 0), reused 0 (delta 0), pack-reused 334\u001b[K\n",
            "Receiving objects: 100% (334/334), 24.77 MiB | 21.73 MiB/s, done.\n",
            "Resolving deltas: 100% (212/212), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8YVW8lK9aIJ",
        "outputId": "219bd263-3597-4662-acf7-bac2c28ee959"
      },
      "source": [
        "!ls ./input/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.csv  test.csv  train.csv  vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYFHCK4r9cFn",
        "outputId": "505a67aa-6652-4e14-e6bf-71f701ab7058"
      },
      "source": [
        "!head -5 ./input/train.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿sentence1,sentence2,label\n",
            "用微信都6年，微信没有微粒贷功能,4。  号码来微粒贷,0\n",
            "微信消费算吗,还有多少钱没还,0\n",
            "交易密码忘记了找回密码绑定的手机卡也掉了,怎么最近安全老是要改密码呢好麻烦,0\n",
            "你好 我昨天晚上申请的没有打电话给我 今天之内一定会打吗？,什么时候可以到账,0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGr62DLFIWka"
      },
      "source": [
        "# 首先设置一下本模型中要用到的具体的参数\r\n",
        "# 设置基本的参数\r\n",
        "base_params = {\r\n",
        "    'num_classes':2,               # 类别   \r\n",
        "    'max_features':1700,             # 嵌入层的最大词数\r\n",
        "    'embed_size':200,              # 嵌入维度\r\n",
        "    'filters':300,\r\n",
        "    'kernel_size':3,\r\n",
        "    'strides':1,\r\n",
        "    'padding':'same',\r\n",
        "    'conv_activation_func':'relu',\r\n",
        "    'embedding_matrix':[],\r\n",
        "    'w_initializer':'random_uniform',\r\n",
        "    'b_initializer':'zeros',\r\n",
        "    'dropout_rate':0.2,\r\n",
        "    'mlp_activation_func':'relu',\r\n",
        "    'mlp_num_layers':1,\r\n",
        "    'mlp_num_units':128,\r\n",
        "    'mlp_num_fan_out':128,\r\n",
        "    'input_shapes':[(64,),(64,)],         # 每句话填充后的最大长度\r\n",
        "    'task':'Classification',\r\n",
        "}\r\n",
        "\r\n",
        "bimpm_params = base_params\r\n",
        "bimpm_params['mp_dim'] = 12\r\n",
        "bimpm_params['lstm_units'] = 64\r\n",
        "bimpm_params['dropout_rate'] = 0.2\r\n",
        "bimpm_params['embed_size'] = 100\r\n",
        "base_params = bimpm_params"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_2hCIGv7j1h"
      },
      "source": [
        "# 数据输入与转换\r\n",
        "由于BiMPM算法对于中文要求的输入是字向量，因为字相比于词，数量大大减少"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFGtdcHeRdLt"
      },
      "source": [
        "#首先把文本中所有的字统计出来，制作出字表\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv('input/train.csv')\r\n",
        "p = df['sentence1'].values\r\n",
        "h = df['sentence2'].values\r\n",
        "p_seg = list(map(lambda x: list(x.replace(\" \",\"\")), p))\r\n",
        "h_seg = list(map(lambda x: list(x.replace(\" \",\"\")), h))\r\n",
        "common_texts = []\r\n",
        "common_texts.extend(p_seg)\r\n",
        "common_texts.extend(h_seg)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df = pd.read_csv('input/dev.csv')\r\n",
        "p = df['sentence1'].values\r\n",
        "h = df['sentence2'].values\r\n",
        "p_seg = list(map(lambda x: list(x.replace(\" \",\"\")), p))\r\n",
        "h_seg = list(map(lambda x: list(x.replace(\" \",\"\")), h))\r\n",
        "common_texts.extend(p_seg)\r\n",
        "common_texts.extend(h_seg)\r\n",
        "\r\n",
        "df = pd.read_csv('input/test.csv')\r\n",
        "p = df['sentence1'].values\r\n",
        "h = df['sentence2'].values\r\n",
        "p_seg = list(map(lambda x: list(x.replace(\" \",\"\")), p))\r\n",
        "h_seg = list(map(lambda x: list(x.replace(\" \",\"\")), h))\r\n",
        "common_texts.extend(p_seg)\r\n",
        "common_texts.extend(h_seg)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsti40c-TCCs",
        "outputId": "7e0df78d-77a1-429a-9a2d-3d0c1f7fbdd5"
      },
      "source": [
        "common_texts[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['用',\n",
              " '微',\n",
              " '信',\n",
              " '都',\n",
              " '6',\n",
              " '年',\n",
              " '，',\n",
              " '微',\n",
              " '信',\n",
              " '没',\n",
              " '有',\n",
              " '微',\n",
              " '粒',\n",
              " '贷',\n",
              " '功',\n",
              " '能']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YhdaWqLTGDK"
      },
      "source": [
        "#使用set来统计不同的字\r\n",
        "char_set = set()\r\n",
        "for sample in common_texts:\r\n",
        "    for char in sample:\r\n",
        "        char_set.add(char)\r\n",
        "with open('input/char_vocab.txt','w',encoding='utf8') as f:\r\n",
        "    f.write(\"\\n\".join(sorted(list(char_set),reverse=True)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR6tcyF-_WX3"
      },
      "source": [
        "#首先读取训练集的数据\r\n",
        "df = pd.read_csv('input/train.csv')\r\n",
        "p = df['sentence1'].values\r\n",
        "h = df['sentence2'].values\r\n",
        "label = df['label'].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0ZqWsoi_8oW"
      },
      "source": [
        "#写一个shuffle函数，将里面的数据随机打乱\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def shuffle(*arrs):\r\n",
        "    \"\"\" shuffle\r\n",
        "\r\n",
        "    Shuffle 数据\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "        *arrs: 数组数据\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        shuffle后的数据\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    arrs = list(arrs)\r\n",
        "    for i, arr in enumerate(arrs):\r\n",
        "        assert len(arrs[0]) == len(arrs[i])\r\n",
        "        arrs[i] = np.array(arr)\r\n",
        "    p = np.random.permutation(len(arrs[0]))\r\n",
        "    return tuple(arr[p] for arr in arrs)\r\n",
        "\r\n",
        "p, h, label = shuffle(p, h, label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rBZEU0wATTt"
      },
      "source": [
        "#接下来要将字转换成对应的数字，所以需要先对字表中的字编码\r\n",
        "# 加载字典\r\n",
        "def load_char_vocab():\r\n",
        "    vocab = [line.strip() for line in open('./input/char_vocab.txt', encoding='utf-8').readlines()]\r\n",
        "    word2idx = {word: index for index, word in enumerate(vocab,start=1)}\r\n",
        "    idx2word = {index: word for index, word in enumerate(vocab,start=1)}\r\n",
        "    return word2idx, idx2word\r\n",
        "\r\n",
        "word2idx, idx2word = load_char_vocab()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvPQOivdBOyd"
      },
      "source": [
        "#利用字典进行转换\r\n",
        "p_list, h_list = [], []\r\n",
        "for p_sentence, h_sentence in zip(p, h):\r\n",
        "    p = [word2idx[word.lower()] for word in p_sentence if len(word.strip()) > 0 and word.lower() in word2idx.keys()]\r\n",
        "    h = [word2idx[word.lower()] for word in h_sentence if len(word.strip()) > 0 and word.lower() in word2idx.keys()]\r\n",
        "\r\n",
        "    p_list.append(p)\r\n",
        "    h_list.append(h)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETvIrVg5Bp3L",
        "outputId": "513b541f-b694-4382-f696-fc7a5a8dbda4"
      },
      "source": [
        "#看一下编码后的句子\r\n",
        "p_list[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1010, 179]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hhKZdCRByf2"
      },
      "source": [
        "#接下来需要写一个pad_sequences函数，将每句话都填充到相同长度，不足的补零\r\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='post',\r\n",
        "                  truncating='post', value=0.):\r\n",
        "    \"\"\" pad_sequences\r\n",
        "\r\n",
        "    把序列长度转变为一样长的，如果设置了maxlen则长度统一为maxlen，如果没有设置则默认取\r\n",
        "    最大的长度。填充和截取包括两种方法，post与pre，post指从尾部开始处理，pre指从头部\r\n",
        "    开始处理，默认都是从尾部开始。\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "        sequences: 序列\r\n",
        "        maxlen: int 最大长度\r\n",
        "        dtype: 转变后的数据类型\r\n",
        "        padding: 填充方法'pre' or 'post'\r\n",
        "        truncating: 截取方法'pre' or 'post'\r\n",
        "        value: float 填充的值\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        x: numpy array 填充后的序列维度为 (number_of_sequences, maxlen)\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    lengths = [len(s) for s in sequences]\r\n",
        "\r\n",
        "    nb_samples = len(sequences)\r\n",
        "    if maxlen is None:\r\n",
        "        maxlen = np.max(lengths)\r\n",
        "\r\n",
        "    x = (np.ones((nb_samples, maxlen)) * value).astype(dtype)\r\n",
        "    for idx, s in enumerate(sequences):\r\n",
        "        if len(s) == 0:\r\n",
        "            continue  # empty list was found\r\n",
        "        if truncating == 'pre':\r\n",
        "            trunc = s[-maxlen:]\r\n",
        "        elif truncating == 'post':\r\n",
        "            trunc = s[:maxlen]\r\n",
        "        else:\r\n",
        "            raise ValueError(\"Truncating type '%s' not understood\" % padding)\r\n",
        "\r\n",
        "        if padding == 'post':\r\n",
        "            x[idx, :len(trunc)] = trunc\r\n",
        "        elif padding == 'pre':\r\n",
        "            x[idx, -len(trunc):] = trunc\r\n",
        "        else:\r\n",
        "            raise ValueError(\"Padding type '%s' not understood\" % padding)\r\n",
        "    return x\r\n",
        "\r\n",
        "p_list = pad_sequences(p_list, maxlen=base_params['input_shapes'][0][0])\r\n",
        "h_list = pad_sequences(h_list, maxlen=base_params['input_shapes'][0][0])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chfb7uH5CbGu",
        "outputId": "3a715beb-12be-4465-d64a-a8eae0cb8c20"
      },
      "source": [
        "#看一下编码后的句子\r\n",
        "p_list[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1010,  179,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE1ELQ2zCeHE"
      },
      "source": [
        "#将训练集数据制作成特征和标签\r\n",
        "import keras\r\n",
        "p = p_list\r\n",
        "h = h_list\r\n",
        "y = label\r\n",
        "x = [p,h]\r\n",
        "y = keras.utils.to_categorical(y,num_classes=2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj_UQWmcDows"
      },
      "source": [
        "#对验证集进行相同的操作\r\n",
        "df = pd.read_csv('input/dev.csv')\r\n",
        "p = df['sentence1'].values\r\n",
        "h = df['sentence2'].values\r\n",
        "label = df['label'].values\r\n",
        "\r\n",
        "p, h, label = shuffle(p, h, label)\r\n",
        "word2idx, idx2word = load_char_vocab()\r\n",
        "\r\n",
        "#利用字典进行转换\r\n",
        "p_list, h_list = [], []\r\n",
        "for p_sentence, h_sentence in zip(p, h):\r\n",
        "    p = [word2idx[word.lower()] for word in p_sentence if len(word.strip()) > 0 and word.lower() in word2idx.keys()]\r\n",
        "    h = [word2idx[word.lower()] for word in h_sentence if len(word.strip()) > 0 and word.lower() in word2idx.keys()]\r\n",
        "\r\n",
        "    p_list.append(p)\r\n",
        "    h_list.append(h)\r\n",
        "\r\n",
        "p_list = pad_sequences(p_list, maxlen=base_params['input_shapes'][0][0])\r\n",
        "h_list = pad_sequences(h_list, maxlen=base_params['input_shapes'][0][0])\r\n",
        "\r\n",
        "p_eval = p_list\r\n",
        "h_eval = h_list\r\n",
        "y_eval = label\r\n",
        "x_eval = [p_eval, h_eval]\r\n",
        "y_eval = keras.utils.to_categorical(y_eval,num_classes=2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPI-TsrHEi3E"
      },
      "source": [
        "#对测试集进行相同的操作,注意这里的测试集是有标签的，便于比较算法效果，测试集并不参与训练过程\r\n",
        "df = pd.read_csv('input/test.csv')\r\n",
        "p = df['sentence1'].values\r\n",
        "h = df['sentence2'].values\r\n",
        "label = df['label'].values\r\n",
        "\r\n",
        "p, h, label = shuffle(p, h, label)\r\n",
        "word2idx, idx2word = load_char_vocab()\r\n",
        "\r\n",
        "#利用字典进行转换\r\n",
        "p_list, h_list = [], []\r\n",
        "for p_sentence, h_sentence in zip(p, h):\r\n",
        "    p = [word2idx[word.lower()] for word in p_sentence if len(word.strip()) > 0 and word.lower() in word2idx.keys()]\r\n",
        "    h = [word2idx[word.lower()] for word in h_sentence if len(word.strip()) > 0 and word.lower() in word2idx.keys()]\r\n",
        "\r\n",
        "    p_list.append(p)\r\n",
        "    h_list.append(h)\r\n",
        "\r\n",
        "p_list = pad_sequences(p_list, maxlen=base_params['input_shapes'][0][0])\r\n",
        "h_list = pad_sequences(h_list, maxlen=base_params['input_shapes'][0][0])\r\n",
        "\r\n",
        "p_test = p_list\r\n",
        "h_test = h_list\r\n",
        "y_test = label\r\n",
        "x_test = [p_test, h_test]\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=2)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n54VFQ3RFU7K"
      },
      "source": [
        "# 建立模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BRLWEo6FY7b"
      },
      "source": [
        "模型建立参考了：https://github.com/wangle1218/deep_text_matching  与match Zoo：https://github.com/NTMC-Community/MatchZoo  \r\n",
        "模型的结构在build函数里，包括输入层，嵌入层，匹配层，卷积和池化的堆叠，输出层，可以看到MatchPyramid是基于交互的方法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXNrJyaHTaZB"
      },
      "source": [
        "from keras.engine import Layer, InputSpec\r\n",
        "from keras.layers import Flatten\r\n",
        "import tensorflow as tf\r\n",
        "import keras.backend as K\r\n",
        "import keras\r\n",
        "\r\n",
        "\r\n",
        "class MatchingLayer(Layer):\r\n",
        "    \"\"\"\r\n",
        "    Layer that computes a matching matrix between samples in two tensors.\r\n",
        "    :param normalize: Whether to L2-normalize samples along the\r\n",
        "        dot product axis before taking the dot product.\r\n",
        "        If set to True, then the output of the dot product\r\n",
        "        is the cosine proximity between the two samples.\r\n",
        "    :param matching_type: the similarity function for matching\r\n",
        "    :param kwargs: Standard layer keyword arguments.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, normalize: bool = False,\r\n",
        "                 matching_type: str = 'dot', **kwargs):\r\n",
        "        \"\"\":class:`MatchingLayer` constructor.\"\"\"\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self._normalize = normalize\r\n",
        "        self._validate_matching_type(matching_type)\r\n",
        "        self._matching_type = matching_type\r\n",
        "        self._shape1 = None\r\n",
        "        self._shape2 = None\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _validate_matching_type(cls, matching_type: str = 'dot'):\r\n",
        "        valid_matching_type = ['dot', 'mul', 'plus', 'minus', 'concat']\r\n",
        "        if matching_type not in valid_matching_type:\r\n",
        "            raise ValueError(f\"{matching_type} is not a valid matching type, \"\r\n",
        "                             f\"{valid_matching_type} expected.\")\r\n",
        "\r\n",
        "    def build(self, input_shape: list):\r\n",
        "        \"\"\"\r\n",
        "        Build the layer.\r\n",
        "        :param input_shape: the shapes of the input tensors,\r\n",
        "            for MatchingLayer we need tow input tensors.\r\n",
        "        \"\"\"\r\n",
        "        # Used purely for shape validation.\r\n",
        "        if not isinstance(input_shape, list) or len(input_shape) != 2:\r\n",
        "            raise ValueError('A `MatchingLayer` layer should be called '\r\n",
        "                             'on a list of 2 inputs.')\r\n",
        "        self._shape1 = input_shape[0]\r\n",
        "        self._shape2 = input_shape[1]\r\n",
        "        for idx in 0, 2:\r\n",
        "            if self._shape1[idx] != self._shape2[idx]:\r\n",
        "                raise ValueError(\r\n",
        "                    'Incompatible dimensions: '\r\n",
        "                    f'{self._shape1[idx]} != {self._shape2[idx]}.'\r\n",
        "                    f'Layer shapes: {self._shape1}, {self._shape2}.'\r\n",
        "                )\r\n",
        "\r\n",
        "    def call(self, inputs: list, **kwargs):\r\n",
        "        \"\"\"\r\n",
        "        The computation logic of MatchingLayer.\r\n",
        "        :param inputs: two input tensors.\r\n",
        "        \"\"\"\r\n",
        "        x1 = inputs[0]\r\n",
        "        x2 = inputs[1]\r\n",
        "        if self._matching_type == 'dot':\r\n",
        "            if self._normalize:\r\n",
        "                x1 = tf.math.l2_normalize(x1, axis=2)\r\n",
        "                x2 = tf.math.l2_normalize(x2, axis=2)\r\n",
        "            return tf.expand_dims(tf.einsum('abd,acd->abc', x1, x2), 3)\r\n",
        "        else:\r\n",
        "            if self._matching_type == 'mul':\r\n",
        "                def func(x, y):\r\n",
        "                    return x * y\r\n",
        "            elif self._matching_type == 'plus':\r\n",
        "                def func(x, y):\r\n",
        "                    return x + y\r\n",
        "            elif self._matching_type == 'minus':\r\n",
        "                def func(x, y):\r\n",
        "                    return x - y\r\n",
        "            elif self._matching_type == 'concat':\r\n",
        "                def func(x, y):\r\n",
        "                    return tf.concat([x, y], axis=3)\r\n",
        "            else:\r\n",
        "                raise ValueError(f\"Invalid matching type.\"\r\n",
        "                                 f\"{self._matching_type} received.\"\r\n",
        "                                 f\"Mut be in `dot`, `mul`, `plus`, \"\r\n",
        "                                 f\"`minus` and `concat`.\")\r\n",
        "            x1_exp = tf.stack([x1] * self._shape2[1], 2)\r\n",
        "            x2_exp = tf.stack([x2] * self._shape1[1], 1)\r\n",
        "            return func(x1_exp, x2_exp)\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape: list) -> tuple:\r\n",
        "        \"\"\"\r\n",
        "        Calculate the layer output shape.\r\n",
        "        :param input_shape: the shapes of the input tensors,\r\n",
        "            for MatchingLayer we need tow input tensors.\r\n",
        "        \"\"\"\r\n",
        "        if not isinstance(input_shape, list) or len(input_shape) != 2:\r\n",
        "            raise ValueError('A `MatchingLayer` layer should be called '\r\n",
        "                             'on a list of 2 inputs.')\r\n",
        "        shape1 = list(input_shape[0])\r\n",
        "        shape2 = list(input_shape[1])\r\n",
        "        if len(shape1) != 3 or len(shape2) != 3:\r\n",
        "            raise ValueError('A `MatchingLayer` layer should be called '\r\n",
        "                             'on 2 inputs with 3 dimensions.')\r\n",
        "        if shape1[0] != shape2[0] or shape1[2] != shape2[2]:\r\n",
        "            raise ValueError('A `MatchingLayer` layer should be called '\r\n",
        "                             'on 2 inputs with same 0,2 dimensions.')\r\n",
        "\r\n",
        "        if self._matching_type in ['mul', 'plus', 'minus']:\r\n",
        "            return shape1[0], shape1[1], shape2[1], shape1[2]\r\n",
        "        elif self._matching_type == 'dot':\r\n",
        "            return shape1[0], shape1[1], shape2[1], 1\r\n",
        "        elif self._matching_type == 'concat':\r\n",
        "            return shape1[0], shape1[1], shape2[1], shape1[2] + shape2[2]\r\n",
        "        else:\r\n",
        "            raise ValueError(f\"Invalid `matching_type`.\"\r\n",
        "                             f\"{self._matching_type} received.\"\r\n",
        "                             f\"Must be in `mul`, `plus`, `minus` \"\r\n",
        "                             f\"`dot` and `concat`.\")\r\n",
        "\r\n",
        "    def get_config(self) -> dict:\r\n",
        "        \"\"\"Get the config dict of MatchingLayer.\"\"\"\r\n",
        "        config = {\r\n",
        "            'normalize': self._normalize,\r\n",
        "            'matching_type': self._matching_type,\r\n",
        "        }\r\n",
        "        base_config = super(MatchingLayer, self).get_config()\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "class MultiPerspective(Layer):\r\n",
        "    \"\"\"Multi-perspective Matching Layer.\r\n",
        "    # Arguments\r\n",
        "        mp_dim: single forward/backward multi-perspective dimention\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, mp_dim, epsilon=1e-6, **kwargs):\r\n",
        "        self.mp_dim = mp_dim\r\n",
        "        self.epsilon = 1e-6\r\n",
        "        self.strategy = 4\r\n",
        "        super(MultiPerspective, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        if isinstance(input_shape, list):\r\n",
        "            input_shape = input_shape[0]\r\n",
        "        embedding_size = int(input_shape[-1] / 2)\r\n",
        "        # Create a trainable weight variable for this layer.\r\n",
        "        # input_shape is bidirectional RNN input shape\r\n",
        "        # kernel shape (mp_dim * 2 * self.strategy, embedding_size)\r\n",
        "        self.kernel = self.add_weight(name='kernel',\r\n",
        "                                    shape = (self.mp_dim,\r\n",
        "                                       embedding_size * 2 * self.strategy),\r\n",
        "                                    initializer='glorot_uniform',\r\n",
        "                                    trainable=True)\r\n",
        "        self.kernel_full_fw = self.kernel[:, :embedding_size]\r\n",
        "        self.kernel_full_bw = self.kernel[:, embedding_size: embedding_size * 2]\r\n",
        "        self.kernel_attentive_fw = self.kernel[:, embedding_size * 2: embedding_size * 3]\r\n",
        "        self.kernel_attentive_bw = self.kernel[:, embedding_size * 3: embedding_size * 4]\r\n",
        "        self.kernel_max_attentive_fw = self.kernel[:, embedding_size * 4: embedding_size * 5]\r\n",
        "        self.kernel_max_attentive_bw = self.kernel[:, embedding_size * 5: embedding_size * 6]\r\n",
        "        self.kernel_max_pool_fw = self.kernel[:, embedding_size * 6: embedding_size * 7]\r\n",
        "        self.kernel_max_pool_bw = self.kernel[:, embedding_size * 7:]\r\n",
        "        self.built = True\r\n",
        "        super(MultiPerspective, self).build(input_shape)\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        if isinstance(input_shape, list):\r\n",
        "            input_shape = input_shape[0]\r\n",
        "        return (input_shape[0], input_shape[1], self.mp_dim * 2 * self.strategy)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {'mp_dim': self.mp_dim,\r\n",
        "                  'epsilon': self.epsilon}\r\n",
        "        base_config = super(MultiPerspective, self).get_config()\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        # h1, h2: bidirectional LSTM hidden states, include forward and backward states\r\n",
        "        #         (batch_size, timesteps, embedding_size * 2)\r\n",
        "        h1 = inputs[0]\r\n",
        "        h2 = inputs[1]\r\n",
        "        embedding_size = int(K.int_shape(h1)[-1] / 2)\r\n",
        "        h1_fw = h1[:, :, :embedding_size]\r\n",
        "        h1_bw = h1[:, :, embedding_size:]\r\n",
        "        h2_fw = h2[:, :, :embedding_size]\r\n",
        "        h2_bw = h2[:, :, embedding_size:]\r\n",
        "\r\n",
        "        # 4 matching strategy\r\n",
        "        list_matching = []\r\n",
        "\r\n",
        "        # full matching ops\r\n",
        "        matching_fw = self._full_matching(h1_fw, h2_fw, self.kernel_full_fw)\r\n",
        "        matching_bw = self._full_matching(h1_bw, h2_bw, self.kernel_full_bw)\r\n",
        "        list_matching.extend([matching_fw, matching_bw])\r\n",
        "\r\n",
        "        # cosine matrix\r\n",
        "        cosine_matrix_fw = self._cosine_matrix(h1_fw, h2_fw)\r\n",
        "        cosine_matrix_bw = self._cosine_matrix(h1_bw, h2_bw)\r\n",
        "\r\n",
        "        # attentive matching ops\r\n",
        "        matching_fw = self._attentive_matching(\r\n",
        "            h1_fw, h2_fw, cosine_matrix_fw, self.kernel_attentive_fw)\r\n",
        "        matching_bw = self._attentive_matching(\r\n",
        "            h1_bw, h2_bw, cosine_matrix_bw, self.kernel_attentive_bw)\r\n",
        "        list_matching.extend([matching_fw, matching_bw])\r\n",
        "\r\n",
        "        # max attentive matching ops\r\n",
        "        matching_fw = self._max_attentive_matching(\r\n",
        "            h1_fw, h2_fw, cosine_matrix_fw, self.kernel_max_attentive_fw)\r\n",
        "        matching_bw = self._max_attentive_matching(\r\n",
        "            h1_bw, h2_bw, cosine_matrix_bw, self.kernel_max_attentive_bw)\r\n",
        "        list_matching.extend([matching_fw, matching_bw])\r\n",
        "\r\n",
        "        # max pooling matching ops\r\n",
        "        matching_fw = self._max_pooling_matching(h1_fw, h2_fw, self.kernel_max_pool_fw)\r\n",
        "        matching_bw = self._max_pooling_matching(h1_bw, h2_bw, self.kernel_max_pool_bw)\r\n",
        "        list_matching.extend([matching_fw, matching_bw])\r\n",
        "\r\n",
        "        return K.concatenate(list_matching, axis=-1)\r\n",
        "    \r\n",
        "    def _cosine_similarity(self, x1, x2):\r\n",
        "        \"\"\"Compute cosine similarity.\r\n",
        "        # Arguments:\r\n",
        "            x1: (..., embedding_size)\r\n",
        "            x2: (..., embedding_size)\r\n",
        "        \"\"\"\r\n",
        "        cos = K.sum(x1 * x2, axis=-1)\r\n",
        "        x1_norm = K.sqrt(K.maximum(K.sum(K.square(x1), axis=-1), self.epsilon))\r\n",
        "        x2_norm = K.sqrt(K.maximum(K.sum(K.square(x2), axis=-1), self.epsilon))\r\n",
        "        cos = cos / x1_norm / x2_norm\r\n",
        "        return cos\r\n",
        "\r\n",
        "    def _cosine_matrix(self, x1, x2):\r\n",
        "        \"\"\"Cosine similarity matrix.\r\n",
        "        Calculate the cosine similarities between each forward (or backward)\r\n",
        "        contextual embedding h_i_p and every forward (or backward)\r\n",
        "        contextual embeddings of the other sentence\r\n",
        "        # Arguments\r\n",
        "            x1: (batch_size, x1_timesteps, embedding_size)\r\n",
        "            x2: (batch_size, x2_timesteps, embedding_size)\r\n",
        "        # Output shape\r\n",
        "            (batch_size, x1_timesteps, x2_timesteps)\r\n",
        "        \"\"\"\r\n",
        "        # expand h1 shape to (batch_size, x1_timesteps, 1, embedding_size)\r\n",
        "        x1 = K.expand_dims(x1, axis=2)\r\n",
        "        # expand x2 shape to (batch_size, 1, x2_timesteps, embedding_size)\r\n",
        "        x2 = K.expand_dims(x2, axis=1)\r\n",
        "        # cosine matrix (batch_size, h1_timesteps, h2_timesteps)\r\n",
        "        cos_matrix = self._cosine_similarity(x1, x2)\r\n",
        "        return cos_matrix\r\n",
        "\r\n",
        "    def _mean_attentive_vectors(self, x2, cosine_matrix):\r\n",
        "        \"\"\"Mean attentive vectors.\r\n",
        "        Calculate mean attentive vector for the entire sentence by weighted\r\n",
        "        summing all the contextual embeddings of the entire sentence\r\n",
        "        # Arguments\r\n",
        "            x2: sequence vectors, (batch_size, x2_timesteps, embedding_size)\r\n",
        "            cosine_matrix: cosine similarities matrix of x1 and x2,\r\n",
        "                           (batch_size, x1_timesteps, x2_timesteps)\r\n",
        "        # Output shape\r\n",
        "            (batch_size, x1_timesteps, embedding_size)\r\n",
        "        \"\"\"\r\n",
        "        # (batch_size, x1_timesteps, x2_timesteps, 1)\r\n",
        "        expanded_cosine_matrix = K.expand_dims(cosine_matrix, axis=-1)\r\n",
        "        # (batch_size, 1, x2_timesteps, embedding_size)\r\n",
        "        x2 = K.expand_dims(x2, axis=1)\r\n",
        "        # (batch_size, x1_timesteps, embedding_size)\r\n",
        "        weighted_sum = K.sum(expanded_cosine_matrix * x2, axis=2)\r\n",
        "        # (batch_size, x1_timesteps, 1)\r\n",
        "        sum_cosine = K.expand_dims(K.sum(cosine_matrix, axis=-1) + self.epsilon, axis=-1)\r\n",
        "        # (batch_size, x1_timesteps, embedding_size)\r\n",
        "        attentive_vector = weighted_sum / sum_cosine\r\n",
        "        return attentive_vector\r\n",
        "\r\n",
        "    def _max_attentive_vectors(self, x2, cosine_matrix):\r\n",
        "        \"\"\"Max attentive vectors.\r\n",
        "        Calculate max attentive vector for the entire sentence by picking\r\n",
        "        the contextual embedding with the highest cosine similarity\r\n",
        "        as the attentive vector.\r\n",
        "        # Arguments\r\n",
        "            x2: sequence vectors, (batch_size, x2_timesteps, embedding_size)\r\n",
        "            cosine_matrix: cosine similarities matrix of x1 and x2,\r\n",
        "                           (batch_size, x1_timesteps, x2_timesteps)\r\n",
        "        # Output shape\r\n",
        "            (batch_size, x1_timesteps, embedding_size)\r\n",
        "        \"\"\"\r\n",
        "        # (batch_size, x1_timesteps)\r\n",
        "        max_x2_step = K.argmax(cosine_matrix, axis=-1)\r\n",
        "\r\n",
        "        embedding_size = K.int_shape(x2)[-1]\r\n",
        "        timesteps = K.int_shape(max_x2_step)[-1]\r\n",
        "        if timesteps is None:\r\n",
        "            timesteps = K.shape(max_x2_step)[-1]\r\n",
        "\r\n",
        "        # collapse time dimension and batch dimension together\r\n",
        "        # collapse x2 to (batch_size * x2_timestep, embedding_size)\r\n",
        "        x2 = K.reshape(x2, (-1, embedding_size))\r\n",
        "        # collapse max_x2_step to (batch_size * h1_timesteps)\r\n",
        "        max_x2_step = K.reshape(max_x2_step, (-1,))\r\n",
        "        # (batch_size * x1_timesteps, embedding_size)\r\n",
        "        max_x2 = K.gather(x2, max_x2_step)\r\n",
        "        # reshape max_x2, (batch_size, x1_timesteps, embedding_size)\r\n",
        "        attentive_vector = K.reshape(max_x2, K.stack([-1, timesteps, embedding_size]))\r\n",
        "        return attentive_vector\r\n",
        "\r\n",
        "    def _time_distributed_multiply(self, x, w):\r\n",
        "        \"\"\"Element-wise multiply vector and weights.\r\n",
        "        # Arguments\r\n",
        "            x: sequence of hidden states, (batch_size, ?, embedding_size)\r\n",
        "            w: weights of one matching strategy of one direction,\r\n",
        "               (mp_dim, embedding_size)\r\n",
        "        # Output shape\r\n",
        "            (?, mp_dim, embedding_size)\r\n",
        "        \"\"\"\r\n",
        "        # dimension of vector\r\n",
        "        n_dim = K.ndim(x)\r\n",
        "        embedding_size = K.int_shape(x)[-1]\r\n",
        "        timesteps = K.int_shape(x)[1]\r\n",
        "        if timesteps is None:\r\n",
        "            timesteps = K.shape(x)[1]\r\n",
        "\r\n",
        "        # collapse time dimension and batch dimension together\r\n",
        "        x = K.reshape(x, (-1, embedding_size))\r\n",
        "        # reshape to (?, 1, embedding_size)\r\n",
        "        x = K.expand_dims(x, axis=1)\r\n",
        "        # reshape weights to (1, mp_dim, embedding_size)\r\n",
        "        w = K.expand_dims(w, axis=0)\r\n",
        "        # element-wise multiply\r\n",
        "        x = x * w\r\n",
        "        # reshape to original shape\r\n",
        "        if n_dim == 3:\r\n",
        "            x = K.reshape(x, K.stack([-1, timesteps, self.mp_dim, embedding_size]))\r\n",
        "            x.set_shape([None, None, None, embedding_size])\r\n",
        "        elif n_dim == 2:\r\n",
        "            x = K.reshape(x, K.stack([-1, self.mp_dim, embedding_size]))\r\n",
        "            x.set_shape([None, None, embedding_size])\r\n",
        "        return x\r\n",
        "\r\n",
        "    def _full_matching(self, h1, h2, w):\r\n",
        "        \"\"\"Full matching operation.\r\n",
        "        # Arguments\r\n",
        "            h1: (batch_size, h1_timesteps, embedding_size)\r\n",
        "            h2: (batch_size, h2_timesteps, embedding_size)\r\n",
        "            w: weights of one direction, (mp_dim, embedding_size)\r\n",
        "        # Output shape\r\n",
        "            (batch_size, h1_timesteps, mp_dim)\r\n",
        "        \"\"\"\r\n",
        "        # h2 forward last step hidden vector, (batch_size, embedding_size)\r\n",
        "        h2_last_state = h2[:, -1, :]\r\n",
        "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\r\n",
        "        h1 = self._time_distributed_multiply(h1, w)\r\n",
        "        # h2_last_state * weights, (batch_size, mp_dim, embedding_size)\r\n",
        "        h2 = self._time_distributed_multiply(h2_last_state, w)\r\n",
        "        # reshape to (batch_size, 1, mp_dim, embedding_size)\r\n",
        "        h2 = K.expand_dims(h2, axis=1)\r\n",
        "        # matching vector, (batch_size, h1_timesteps, mp_dim)\r\n",
        "        matching = self._cosine_similarity(h1, h2)\r\n",
        "        return matching\r\n",
        "\r\n",
        "    def _max_pooling_matching(self, h1, h2, w):\r\n",
        "        \"\"\"Max pooling matching operation.\r\n",
        "        # Arguments\r\n",
        "            h1: (batch_size, h1_timesteps, embedding_size)\r\n",
        "            h2: (batch_size, h2_timesteps, embedding_size)\r\n",
        "            w: weights of one direction, (mp_dim, embedding_size)\r\n",
        "        # Output shape\r\n",
        "            (batch_size, h1_timesteps, mp_dim)\r\n",
        "        \"\"\"\r\n",
        "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\r\n",
        "        h1 = self._time_distributed_multiply(h1, w)\r\n",
        "        # h2 * weights, (batch_size, h2_timesteps, mp_dim, embedding_size)\r\n",
        "        h2 = self._time_distributed_multiply(h2, w)\r\n",
        "        # reshape v1 to (batch_size, h1_timesteps, 1, mp_dim, embedding_size)\r\n",
        "        h1 = K.expand_dims(h1, axis=2)\r\n",
        "        # reshape v1 to (batch_size, 1, h2_timesteps, mp_dim, embedding_size)\r\n",
        "        h2 = K.expand_dims(h2, axis=1)\r\n",
        "        # cosine similarity, (batch_size, h1_timesteps, h2_timesteps, mp_dim)\r\n",
        "        cos = self._cosine_similarity(h1, h2)\r\n",
        "        # (batch_size, h1_timesteps, mp_dim)\r\n",
        "        matching = K.max(cos, axis=2)\r\n",
        "        return matching\r\n",
        "\r\n",
        "    def _attentive_matching(self, h1, h2, cosine_matrix, w):\r\n",
        "        \"\"\"Attentive matching operation.\r\n",
        "        # Arguments\r\n",
        "            h1: (batch_size, h1_timesteps, embedding_size)\r\n",
        "            h2: (batch_size, h2_timesteps, embedding_size)\r\n",
        "            cosine_matrix: weights of hidden state h2,\r\n",
        "                          (batch_size, h1_timesteps, h2_timesteps)\r\n",
        "            w: weights of one direction, (mp_dim, embedding_size)\r\n",
        "        # Output shape\r\n",
        "            (batch_size, h1_timesteps, mp_dim)\r\n",
        "        \"\"\"\r\n",
        "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\r\n",
        "        h1 = self._time_distributed_multiply(h1, w)\r\n",
        "        # attentive vector (batch_size, h1_timesteps, embedding_szie)\r\n",
        "        attentive_vec = self._mean_attentive_vectors(h2, cosine_matrix)\r\n",
        "        # attentive_vec * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\r\n",
        "        attentive_vec = self._time_distributed_multiply(attentive_vec, w)\r\n",
        "        # matching vector, (batch_size, h1_timesteps, mp_dim)\r\n",
        "        matching = self._cosine_similarity(h1, attentive_vec)\r\n",
        "        return matching\r\n",
        "\r\n",
        "    def _max_attentive_matching(self, h1, h2, cosine_matrix, w):\r\n",
        "        \"\"\"Max attentive matching operation.\r\n",
        "        # Arguments\r\n",
        "            h1: (batch_size, h1_timesteps, embedding_size)\r\n",
        "            h2: (batch_size, h2_timesteps, embedding_size)\r\n",
        "            cosine_matrix: weights of hidden state h2,\r\n",
        "                          (batch_size, h1_timesteps, h2_timesteps)\r\n",
        "            w: weights of one direction, (mp_dim, embedding_size)\r\n",
        "        # Output shape\r\n",
        "            (batch_size, h1_timesteps, mp_dim)\r\n",
        "        \"\"\"\r\n",
        "        # h1 * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\r\n",
        "        h1 = self._time_distributed_multiply(h1, w)\r\n",
        "        # max attentive vector (batch_size, h1_timesteps, embedding_szie)\r\n",
        "        max_attentive_vec = self._max_attentive_vectors(h2, cosine_matrix)\r\n",
        "        # max_attentive_vec * weights, (batch_size, h1_timesteps, mp_dim, embedding_size)\r\n",
        "        max_attentive_vec = self._time_distributed_multiply(max_attentive_vec, w)\r\n",
        "        # matching vector, (batch_size, h1_timesteps, mp_dim)\r\n",
        "        matching = self._cosine_similarity(h1, max_attentive_vec)\r\n",
        "        return matching"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1C2C1R_Fz1o"
      },
      "source": [
        "from keras.models import Model\r\n",
        "import tensorflow as tf \r\n",
        "\r\n",
        "np.random.seed(1)\r\n",
        "tf.random.set_seed(1)\r\n",
        "\r\n",
        "class BiMPM(object):\r\n",
        "\r\n",
        "    def __init__( self, params):\r\n",
        "        \"\"\"Init.\"\"\"\r\n",
        "        self._params = params\r\n",
        "\r\n",
        "    \r\n",
        "    def make_embedding_layer(self,name='embedding',embed_type='char',**kwargs):   #加载词向量的方法\r\n",
        "\r\n",
        "        def init_embedding(weights=None):\r\n",
        "            if embed_type == \"char\":\r\n",
        "                input_dim = self._params['max_features']\r\n",
        "                output_dim = self._params['embed_size']\r\n",
        "            else:\r\n",
        "                input_dim = self._params['word_max_features']\r\n",
        "                output_dim = self._params['word_embed_size']\r\n",
        "\r\n",
        "            return keras.layers.Embedding(\r\n",
        "                input_dim = input_dim,\r\n",
        "                output_dim = output_dim,\r\n",
        "                trainable = False,\r\n",
        "                name = name,\r\n",
        "                weights = weights,\r\n",
        "                **kwargs)\r\n",
        "\r\n",
        "        if embed_type == \"char\":\r\n",
        "            embed_weights = self._params['embedding_matrix']\r\n",
        "        else:\r\n",
        "            embed_weights = self._params['word_embedding_matrix']\r\n",
        "\r\n",
        "        if embed_weights == []:\r\n",
        "            embedding = init_embedding()\r\n",
        "        else:\r\n",
        "            embedding = init_embedding(weights = [embed_weights])\r\n",
        "\r\n",
        "        return embedding\r\n",
        "\r\n",
        "    def _make_multi_layer_perceptron_layer(self) -> keras.layers.Layer:   #多层感知机方法（全连接）\r\n",
        "        # TODO: do not create new layers for a second call\r\n",
        "        def _wrapper(x):\r\n",
        "            activation = self._params['mlp_activation_func']   #全连接激活函数\r\n",
        "            for _ in range(self._params['mlp_num_layers']):  #全连接层数\r\n",
        "                x = keras.layers.Dense(self._params['mlp_num_units'],   #全连接神经元数\r\n",
        "                                       activation=activation)(x)\r\n",
        "            return keras.layers.Dense(self._params['mlp_num_fan_out'],  #全连接最后一层神经元数\r\n",
        "                                      activation=activation)(x)\r\n",
        "\r\n",
        "        return _wrapper\r\n",
        "\r\n",
        "    def _make_inputs(self) -> list:        #定义输入层，这里只有两个输入，也就是要判断是否匹配的两段文本\r\n",
        "        input_left = keras.layers.Input(\r\n",
        "            name='text_left',\r\n",
        "            shape=self._params['input_shapes'][0]\r\n",
        "        )\r\n",
        "        input_right = keras.layers.Input(\r\n",
        "            name='text_right',\r\n",
        "            shape=self._params['input_shapes'][1]\r\n",
        "        )\r\n",
        "        return [input_left, input_right]\r\n",
        "\r\n",
        "    def _make_output_layer(self) -> keras.layers.Layer:   #定义输出层\r\n",
        "        \"\"\":return: a correctly shaped keras dense layer for model output.\"\"\"\r\n",
        "        task = self._params['task']\r\n",
        "        if task == \"Classification\":\r\n",
        "            return keras.layers.Dense(self._params['num_classes'], activation='softmax')  #分类使用softmax，两个文档输入就分类\r\n",
        "        elif task == \"Ranking\":\r\n",
        "            return keras.layers.Dense(1, activation='linear')          #排序使用linear，多个文档输入就排序\r\n",
        "        else:\r\n",
        "            raise ValueError(f\"{task} is not a valid task type.\"\r\n",
        "                             f\"Must be in `Ranking` and `Classification`.\")\r\n",
        "\r\n",
        "\r\n",
        "    def build(self):\r\n",
        "        input_left, input_right = self._make_inputs()\r\n",
        "\r\n",
        "        # ----- Embedding layer ----- \r\n",
        "        embedding = self.make_embedding_layer()\r\n",
        "        embed_left = embedding(input_left)\r\n",
        "        embed_right = embedding(input_right)\r\n",
        "\r\n",
        "        # ----- Context Representation Layer ----- \r\n",
        "        # rep_left = keras.layers.Bidirectional(keras.layers.LSTM(\r\n",
        "        #     self._params['lstm_units'],\r\n",
        "        #     return_sequences=True,\r\n",
        "        #     dropout=self._params['dropout_rate']\r\n",
        "        # ))(embed_left)\r\n",
        "        # rep_right = keras.layers.Bidirectional(keras.layers.LSTM(\r\n",
        "        #     self._params['lstm_units'],\r\n",
        "        #     return_sequences=True,\r\n",
        "        #     dropout=self._params['dropout_rate']\r\n",
        "        # ))(embed_right)\r\n",
        "\r\n",
        "        bilstm = keras.layers.Bidirectional(keras.layers.LSTM(\r\n",
        "            self._params['lstm_units'],\r\n",
        "            return_sequences=True,\r\n",
        "            dropout=self._params['dropout_rate']\r\n",
        "        ))\r\n",
        "        rep_left = bilstm(embed_left)\r\n",
        "        rep_right = bilstm(embed_right)\r\n",
        "\r\n",
        "        # ----- Matching Layer -----\r\n",
        "        matching_layer = MultiPerspective(self._params['mp_dim'])\r\n",
        "        matching_left = matching_layer([rep_left, rep_right])\r\n",
        "        matching_right = matching_layer([rep_right, rep_left])\r\n",
        "\r\n",
        "        # ----- Aggregation Layer -----\r\n",
        "        agg_left = keras.layers.Bidirectional(keras.layers.LSTM(\r\n",
        "            self._params['lstm_units'],\r\n",
        "            return_sequences=False,\r\n",
        "            dropout=self._params['dropout_rate']\r\n",
        "        ))(matching_left)\r\n",
        "        agg_right = keras.layers.Bidirectional(keras.layers.LSTM(\r\n",
        "            self._params['lstm_units'],\r\n",
        "            return_sequences=False,\r\n",
        "            dropout=self._params['dropout_rate']\r\n",
        "        ))(matching_right)\r\n",
        "\r\n",
        "        aggregation = keras.layers.concatenate([agg_left, agg_right])\r\n",
        "        aggregation = keras.layers.Dropout(rate=self._params['dropout_rate'])(aggregation)\r\n",
        "\r\n",
        "        # ----- Prediction Layer -----\r\n",
        "        inputs = [input_left, input_right]\r\n",
        "        x_out = self._make_output_layer()(aggregation)\r\n",
        "\r\n",
        "        model = keras.Model(inputs=inputs, outputs=x_out)\r\n",
        "\r\n",
        "        return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1e3S0tjNibw"
      },
      "source": [
        "# 加载模型训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-YbfumJH8Uz"
      },
      "source": [
        "params = base_params\r\n",
        "backend = BiMPM(params)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io6Qv_rvJ21Z"
      },
      "source": [
        "model = backend.build()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3iwKspxL4Cp"
      },
      "source": [
        "model.compile(\r\n",
        "        loss='categorical_crossentropy', \r\n",
        "        optimizer='adam', \r\n",
        "        metrics=['accuracy']\r\n",
        "        )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6dVcvRdL9Wf",
        "outputId": "3e806f4c-f3c2-42f0-da6c-92b006518d68"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "text_left (InputLayer)          [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "text_right (InputLayer)         [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 64, 100)      170000      text_left[0][0]                  \n",
            "                                                                 text_right[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 64, 128)      84480       embedding[0][0]                  \n",
            "                                                                 embedding[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multi_perspective (MultiPerspec (None, 64, 96)       6144        bidirectional[0][0]              \n",
            "                                                                 bidirectional[1][0]              \n",
            "                                                                 bidirectional[1][0]              \n",
            "                                                                 bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 128)          82432       multi_perspective[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 128)          82432       multi_perspective[1][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           bidirectional_1[0][0]            \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            514         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 426,002\n",
            "Trainable params: 256,002\n",
            "Non-trainable params: 170,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWz0ftrfMVMg"
      },
      "source": [
        "!mkdir output"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC3Lp-9-MGtf",
        "outputId": "0616b349-449d-4768-a8fb-c505243d3aa0"
      },
      "source": [
        "earlystop = keras.callbacks.EarlyStopping(\r\n",
        "        monitor='val_accuracy', \r\n",
        "        patience=4, \r\n",
        "        verbose=2, \r\n",
        "        mode='max'\r\n",
        "        )\r\n",
        "model_name = 'BiMPM'\r\n",
        "bast_model_filepath = './output/best_%s_model.h5' % model_name\r\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(\r\n",
        "    bast_model_filepath, \r\n",
        "    monitor='val_accuracy', \r\n",
        "    verbose=1, \r\n",
        "    save_best_only=True,\r\n",
        "    mode='max'\r\n",
        "    )\r\n",
        "model.fit(\r\n",
        "    x=x, \r\n",
        "    y=y, \r\n",
        "    batch_size=64, \r\n",
        "    epochs=15, \r\n",
        "    validation_data=(x_eval, y_eval), \r\n",
        "    shuffle=True, \r\n",
        "    callbacks=[earlystop,checkpoint]\r\n",
        "    )"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['multi_perspective/kernel:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['multi_perspective/kernel:0'] when minimizing the loss.\n",
            "1563/1563 [==============================] - 292s 175ms/step - loss: 0.6037 - accuracy: 0.6653 - val_loss: 0.5256 - val_accuracy: 0.7502\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75020, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.5121 - accuracy: 0.7527 - val_loss: 0.5288 - val_accuracy: 0.7415\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.75020\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 274s 175ms/step - loss: 0.4872 - accuracy: 0.7690 - val_loss: 0.5048 - val_accuracy: 0.7620\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.75020 to 0.76200, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 274s 175ms/step - loss: 0.4723 - accuracy: 0.7768 - val_loss: 0.4749 - val_accuracy: 0.7739\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.76200 to 0.77390, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 274s 175ms/step - loss: 0.4565 - accuracy: 0.7869 - val_loss: 0.4542 - val_accuracy: 0.7877\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.77390 to 0.78770, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.4435 - accuracy: 0.7951 - val_loss: 0.4666 - val_accuracy: 0.7814\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.78770\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.4268 - accuracy: 0.8038 - val_loss: 0.4561 - val_accuracy: 0.7899\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.78770 to 0.78990, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.4209 - accuracy: 0.8073 - val_loss: 0.4752 - val_accuracy: 0.7743\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.78990\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.4093 - accuracy: 0.8131 - val_loss: 0.4564 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.78990 to 0.79060, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.3947 - accuracy: 0.8215 - val_loss: 0.4388 - val_accuracy: 0.8019\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.79060 to 0.80190, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 11/15\n",
            "1563/1563 [==============================] - 274s 175ms/step - loss: 0.3906 - accuracy: 0.8256 - val_loss: 0.4548 - val_accuracy: 0.7934\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.80190\n",
            "Epoch 12/15\n",
            "1563/1563 [==============================] - 274s 176ms/step - loss: 0.3880 - accuracy: 0.8260 - val_loss: 0.4403 - val_accuracy: 0.7965\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.80190\n",
            "Epoch 13/15\n",
            "1563/1563 [==============================] - 275s 176ms/step - loss: 0.3920 - accuracy: 0.8244 - val_loss: 0.4550 - val_accuracy: 0.7982\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.80190\n",
            "Epoch 14/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.3749 - accuracy: 0.8326 - val_loss: 0.4404 - val_accuracy: 0.8035\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.80190 to 0.80350, saving model to ./output/best_BiMPM_model.h5\n",
            "Epoch 15/15\n",
            "1563/1563 [==============================] - 273s 175ms/step - loss: 0.3676 - accuracy: 0.8374 - val_loss: 0.4378 - val_accuracy: 0.8031\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.80350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc85e45acc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RinZM_KhNoj_"
      },
      "source": [
        "# 保存模型\r\n",
        "model_frame_path = \"./output/%s_model.json\" % model_name\r\n",
        "model_json = model.to_json()\r\n",
        "with open(model_frame_path, \"w\") as json_file:\r\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjL7-L8RN-8n"
      },
      "source": [
        "# 测试模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS99ULO0N112",
        "outputId": "ae84f5a0-e9dd-4dc1-e163-7b816b9cd8a1"
      },
      "source": [
        "model.load_weights(bast_model_filepath)\r\n",
        "model.compile(\r\n",
        "    loss='categorical_crossentropy', \r\n",
        "    optimizer='adam', \r\n",
        "    metrics=['accuracy']\r\n",
        "    )\r\n",
        "\r\n",
        "loss, acc = model.evaluate(\r\n",
        "    x=x_test, \r\n",
        "    y=y_test, \r\n",
        "    batch_size=128, \r\n",
        "    verbose=1\r\n",
        "    )\r\n",
        "print(\"Test loss:\",loss, \"Test accuracy:\",acc)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 14s 112ms/step - loss: 0.4584 - accuracy: 0.7936\n",
            "Test loss: 0.4510013163089752 Test accuracy: 0.7996000051498413\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}